# *****************************************************************************
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
# ******************************************************************************

FROM ubuntu:18.04

ARG NB_USER="dlab-user"
ENV CONF_FILE="/home/dlab-user/.local/share/jupyter/jupyter_notebook_config.py"
ENV PATH /usr/local/bin:$PATH
ENV LANG C.UTF-8
ENV DEBIAN_FRONTEND=noninteractive


RUN apt-get update && apt-get install -y --no-install-recommends \
                tk-dev wget curl gnupg dirmngr build-essential openssl \
                libreadline-gplv2-dev libncursesw5-dev libssl-dev \
                libsqlite3-dev libgdbm-dev libc6-dev libbz2-dev \
                python python-dev python-pip python-virtualenv \
                python3 python3-dev python3-pip default-jre default-jdk \
  && rm -rf /var/lib/apt/lists/*


RUN useradd -u 1001 -ms /bin/bash $NB_USER


#Installing Python libraries
RUN pip install --upgrade setuptools \
  && pip install tornado==4.5.3 ipython ipython matplotlib==2.0.2 \
   boto3 fabvenv fabric-virtualenv future NumPy==1.14.3 \
   SciPy pandas Sympy Pillow sklearn pyopenssl --no-cache-dir \
  && pip3 install --upgrade setuptools \
  && pip3 install tornado==4.5.3 ipython ipython matplotlib==2.0.2 \
   boto3 fabvenv fabric-virtualenv future NumPy==1.14.3 \
   SciPy pandas Sympy Pillow sklearn --no-cache-dir



#Installing SBT
# Env variables
ENV SCALA_VERSION 2.12.8
ENV SBT_VERSION 1.2.8

# Install Scala
## Piping curl directly in tar
RUN \
  curl -fsL https://downloads.typesafe.com/scala/$SCALA_VERSION/scala-$SCALA_VERSION.tgz | tar xfz - -C /root/ && \
  echo >> /root/.bashrc && \
  echo "export PATH=~/scala-$SCALA_VERSION/bin:$PATH" >> /root/.bashrc

# Install sbt
RUN \
  curl -L -o sbt-$SBT_VERSION.deb https://dl.bintray.com/sbt/debian/sbt-$SBT_VERSION.deb && \
  dpkg -i sbt-$SBT_VERSION.deb && \
  rm sbt-$SBT_VERSION.deb && \
  apt-get update && \
  apt-get install sbt && \
  sbt sbtVersion && \
  mkdir project && \
  echo "scalaVersion := \"${SCALA_VERSION}\"" > build.sbt && \
  echo "sbt.version=${SBT_VERSION}" > project/build.properties && \
  echo "case object Temp" > Temp.scala && \
  sbt compile && \
rm -r project && rm build.sbt && rm Temp.scala && rm -r target



#Installing Jupyter
RUN pip install notebook==jup_version --no-cache-dir \
  && pip install jupyter --no-cache-dir \
  && pip3 install notebook==jup_version --no-cache-dir \
  && pip3 install jupyter --no-cache-dir \
  && rm -rf $CONF_FILE \
  && mkdir -p /home/$NB_USER/.jupyter/custom/ \
  && echo "#notebook-container { width: auto; }" > /home/$NB_USER/.jupyter/custom/custom.css \
  && mkdir -p /mnt/var \
  && chown $NB_USER:$NB_USER /mnt/var \
  && jupyter-kernelspec remove -f python3 || echo "Such kernel doesnt exists" \
  && jupyter-kernelspec remove -f python2 || echo "Such kernel doesnt exists"
COPY jupyter_notebook_config.py /home/$NB_USER/.local/share/jupyter/jupyter_notebook_config.py

#Intsalling local Spark
RUN wget https://archive.apache.org/dist/spark/spark-2.3.2/spark-2.3.2-bin-hadoop2.7.tgz -O \
  /tmp/spark-2.3.2-bin-hadoop2.7.tgz \
  && tar -zxvf /tmp/spark-2.3.2-bin-hadoop2.7.tgz -C /opt/ \
  && mv /opt/spark-2.3.2-bin-hadoop2.7 /opt/spark \
  && mkdir -p /opt/jars/ \
  && wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.4/hadoop-aws-2.7.4.jar -O \
  /opt/jars/hadoop-aws-2.7.4.jar \
  && wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar -O \
  /opt/jars/aws-java-sdk-1.7.4.jar \
  && wget https://maven.twttr.com/com/hadoop/gplcompression/hadoop-lzo/0.4.20/hadoop-lzo-0.4.20.jar -O \
  /opt/jars/hadoop-lzo-0.4.20.jar


#Installing local kernel
RUN mkdir -p /home/$NB_USER/.local/share/jupyter/kernels/py3spark_local/ \
   /home/$NB_USER/.local/share/jupyter/kernels/pyspark_local/
COPY pyspark_local_template.json /tmp/pyspark_local_template.json
COPY py3spark_local_template.json /tmp/py3spark_local_template.json
COPY spark.sh /tmp/spark.sh
RUN /tmp/spark.sh  #this script is needed because docker is not able to handle one of the commands


EXPOSE 8888

#Prepearing Start script
COPY jupyter_run.sh /jupyter_run.sh
RUN  sed -i 's|CONF_PATH|/home/dlab-user/.local/share/jupyter/jupyter_notebook_config.py|' /jupyter_run.sh \
  && chmod +x /jupyter_run.sh \
  && chown -R 1001:1001 /home/$NB_USER/.local/share/jupyter

USER $NB_USER

ENTRYPOINT ["/jupyter_run.sh", "-d"]